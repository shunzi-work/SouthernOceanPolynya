{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter some warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import gsw\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd = pd.read_csv(\"all_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import gcsfs\n",
    "# this only needs to be created once\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../../../data/model/CMIP6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mld(sigma0, lev):\n",
    "    b0 = sigma0[lev].where(~sigma0.isnull()).max(dim = lev)\n",
    "    sigma0_10 = sigma0.interp({lev: 10})\n",
    "    mld0 = sigma0[lev].where(sigma0 - sigma0_10 <= 0.03).max(dim = lev)\n",
    "    mld1 = sigma0[lev].where(sigma0[lev] > mld0).min(dim = lev)\n",
    "    sigma0_cal = sigma0.where((sigma0[lev] >= mld0) & (sigma0[lev] <= mld1))\n",
    "    cal_min = sigma0_cal.min(dim = lev)\n",
    "    cal_max = sigma0_cal.max(dim = lev)\n",
    "    mld2 = (mld1 - mld0)/(cal_max - cal_min) * (sigma0_10 + 0.03 - cal_min) + mld0\n",
    "    mld = xr.where(mld0 < b0, mld2, b0)\n",
    "    return mld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    memory = '8G',\n",
    "    processes = 1,\n",
    "    cores = 2, \n",
    "    nanny = True, \n",
    "    silence_logs = 'error')\n",
    "\n",
    "cluster.scale(32)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != 'GISS-E2-2-H':\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    fpath = 'data_mld/' + f + '.pickle'\n",
    "    # print(new_path)\n",
    "\n",
    "    sf = new_path + \"/so*.nc\"\n",
    "    tf = new_path + \"/thetao*.nc\"\n",
    "    # print(new_sof)\n",
    "    dss = xr.open_mfdataset(sf)\n",
    "    dst = xr.open_mfdataset(tf)\n",
    "\n",
    "    das = dss.so\n",
    "    dat = dst.thetao\n",
    "    \n",
    "    if 'type' in das.coords:\n",
    "        das = das.reset_coords('type', drop = True)\n",
    "        dat = dat.reset_coords('type', drop = True)\n",
    "    \n",
    "    da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "\n",
    "    levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "    if 'units' in das[levname].attrs:\n",
    "        if das[levname].units == 'centimeters':\n",
    "            da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "\n",
    "    if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "        if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "            print(\"{} doesn't have regular grid.\".format(f))\n",
    "            continue\n",
    "        else:\n",
    "            da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "            # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "            pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "            plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "            pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "    else:\n",
    "        da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "        pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "        plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "    \n",
    "    if len(np.shape(pltx)) > 2:\n",
    "        pltx = pltx.isel(time = 0)\n",
    "        plty = plty.isel(time = 0)\n",
    "\n",
    "    da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "    da_mld = cal_mld(da_sep, levname)\n",
    "    \n",
    "    mldmax = da_mld.max(\"time\")\n",
    "    mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "    mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "    \n",
    "    newd = {\"mldmax\":mldmax.load(), \n",
    "            \"mld2kfq\":mld2000_frq.load(), \n",
    "            \"pltx\":pltx, \n",
    "            \"plty\":plty}\n",
    "    \n",
    "    with open(fpath, 'wb') as wf:\n",
    "        pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(\"{}: finished\".format(f))\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != 'IPSL-CM5A2-INCA':\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    all_files = os.listdir(new_path)\n",
    "    s_files = [file for file in all_files if file.startswith(\"so_Omon\")]\n",
    "    t_files = [file for file in all_files if file.startswith(\"thetao_Omon\")]\n",
    "\n",
    "    s_files.sort()\n",
    "    t_files.sort()\n",
    "    \n",
    "    # Iterate through the pairs of files\n",
    "    fn = 0\n",
    "    for s_file, t_file in zip(s_files, t_files):\n",
    "        s_file_path = os.path.join(new_path, s_file)\n",
    "        t_file_path = os.path.join(new_path, t_file)\n",
    "        \n",
    "        s_longname = s_file.split('_')[1:]\n",
    "        t_longname = t_file.split('_')[1:]\n",
    "        \n",
    "        if s_longname == t_longname:\n",
    "\n",
    "            chunksize = {\"time\":24,\n",
    "                         datapd.loc[datapd['source_id'] == f].iloc[0]['xname']:180,\n",
    "                         datapd.loc[datapd['source_id'] == f].iloc[0]['yname']:180}\n",
    "\n",
    "            dss = xr.open_dataset(s_file_path, chunks=chunksize)\n",
    "            dst = xr.open_dataset(t_file_path, chunks=chunksize)\n",
    "\n",
    "            if 'type' in das.coords:\n",
    "                das = das.reset_coords('type', drop = True)\n",
    "                dat = dat.reset_coords('type', drop = True)\n",
    "\n",
    "            levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "\n",
    "            fns = np.linspace(0, len(dss.so.time), 11)\n",
    "            for fn in range(0, len(fns)-1):\n",
    "                savepath = 'data_mld0/' + f + '_' + str(fn) + '.pickle'\n",
    "                if os.path.exists(savepath):\n",
    "                    continue\n",
    "\n",
    "                das = dss.so.isel(time = slice(int(fns[fn]), int(fns[fn+1])))\n",
    "                dat = dst.thetao.isel(time = slice(int(fns[fn]), int(fns[fn+1])))\n",
    "\n",
    "                da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "                \n",
    "                if 'units' in das[levname].attrs:\n",
    "                    if das[levname].units == 'centimeters':\n",
    "                        da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "                        \n",
    "                \n",
    "                if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "                    if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "                        print(\"{} doesn't have regular grid.\".format(f))\n",
    "                        continue\n",
    "                    else:\n",
    "                        da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "                        # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "                        pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "                        plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "                        pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "                else:\n",
    "                    da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "                    pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "                    plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "                \n",
    "                if len(np.shape(pltx)) > 2:\n",
    "                    pltx = pltx.isel(time = 0)\n",
    "                    plty = plty.isel(time = 0)\n",
    "                    \n",
    "                da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "                da_mld = cal_mld(da_sep, levname)\n",
    "                da_mld_new = da_mld.load()\n",
    "                \n",
    "                with open(savepath, 'wb') as wf:\n",
    "                    pickle.dump(da_mld_new, wf, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # else:\n",
    "        #     print(s_longname)\n",
    "        #     print(t_longname)\n",
    "        #     break\n",
    "            # if fn == 0:\n",
    "            #     da_mld_all = da_mld_new.load()\n",
    "            # else:\n",
    "            #     da_mld_all = xr.concat([da_mld_all, da_mld_new], dim=\"time\")\n",
    "            # if fn > 2:\n",
    "            #     break\n",
    "            # fn = fn + 1\n",
    "            \n",
    "        \n",
    "\n",
    "    # mldmax = da_mld.max(\"time\")\n",
    "    # mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "    # mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "    \n",
    "    # newd = {\"mldmax\":mldmax.load(), \n",
    "    #         \"mld2kfq\":mld2000_frq.load(), \n",
    "    #         \"pltx\":pltx, \n",
    "    #         \"plty\":plty}\n",
    "    \n",
    "    # with open(fpath, 'wb') as wf:\n",
    "    #     pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # print(\"{}: finished\".format(f))\n",
    "            \n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != 'CanESM5-1':\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    all_files = os.listdir(new_path)\n",
    "    s_files = [file for file in all_files if file.startswith(\"so_Omon\")]\n",
    "    t_files = [file for file in all_files if file.startswith(\"thetao_Omon\")]\n",
    "\n",
    "    s_files.sort()\n",
    "    t_files.sort()\n",
    "    \n",
    "    # Iterate through the pairs of files\n",
    "    fn = 0\n",
    "    for s_file, t_file in zip(s_files, t_files):\n",
    "        s_file_path = os.path.join(new_path, s_file)\n",
    "        t_file_path = os.path.join(new_path, t_file)\n",
    "        \n",
    "        s_longname = s_file.split('_')[1:]\n",
    "        t_longname = t_file.split('_')[1:]\n",
    "        \n",
    "        if s_longname != t_longname:\n",
    "            print('time span does not match ')\n",
    "            break\n",
    "        else:\n",
    "            dss = xr.open_dataset(s_file_path)\n",
    "            dst = xr.open_dataset(t_file_path)\n",
    "\n",
    "            if 'type' in dss.coords:\n",
    "                das = dss.reset_coords('type', drop = True)\n",
    "                dat = dst.reset_coords('type', drop = True)\n",
    "\n",
    "            levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "\n",
    "            nfns = np.linspace(0, len(dss.so.time), int(len(dss.so.time)/12+1))\n",
    "\n",
    "            for nfn in range(0, len(nfns)-1):\n",
    "                sfn = fn + nfn\n",
    "                savepath = 'data_mld0/' + f + '_' + str(sfn) + '.pickle'\n",
    "                if os.path.exists(savepath):\n",
    "                    continue\n",
    "                print('file {}, number {}, time {} - {}'.format(s_file, sfn, nfns[nfn], nfns[nfn+1]))\n",
    "\n",
    "                das = dss.so.isel(time = slice(int(nfns[nfn]), int(nfns[nfn+1])))\n",
    "                dat = dst.thetao.isel(time = slice(int(nfns[nfn]), int(nfns[nfn+1])))\n",
    "\n",
    "                da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "                \n",
    "                if 'units' in das[levname].attrs:\n",
    "                    if das[levname].units == 'centimeters':\n",
    "                        da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "                        \n",
    "                \n",
    "                if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "                    if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "                        print(\"{} doesn't have regular grid.\".format(f))\n",
    "                        continue\n",
    "                    else:\n",
    "                        da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "                        # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "                        pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "                        plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "                        pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "                else:\n",
    "                    da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "                    pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "                    plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "                \n",
    "                if len(np.shape(pltx)) > 2:\n",
    "                    pltx = pltx.isel(time = 0)\n",
    "                    plty = plty.isel(time = 0)\n",
    "                    \n",
    "                da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "                da_mld = cal_mld(da_sep, levname)\n",
    "                da_mld_new = da_mld.load()\n",
    "                \n",
    "                with open(savepath, 'wb') as wf:\n",
    "                    pickle.dump(da_mld_new, wf, pickle.HIGHEST_PROTOCOL)\n",
    "            fn = sfn + 1\n",
    "\n",
    "        # else:\n",
    "        #     print(s_longname)\n",
    "        #     print(t_longname)\n",
    "        #     break\n",
    "            # if fn == 0:\n",
    "            #     da_mld_all = da_mld_new.load()\n",
    "            # else:\n",
    "            #     da_mld_all = xr.concat([da_mld_all, da_mld_new], dim=\"time\")\n",
    "            # if fn > 2:\n",
    "            #     break\n",
    "            # fn = fn + 1\n",
    "            \n",
    "        \n",
    "\n",
    "    # mldmax = da_mld.max(\"time\")\n",
    "    # mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "    # mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "    \n",
    "    # newd = {\"mldmax\":mldmax.load(), \n",
    "    #         \"mld2kfq\":mld2000_frq.load(), \n",
    "    #         \"pltx\":pltx, \n",
    "    #         \"plty\":plty}\n",
    "    \n",
    "    # with open(fpath, 'wb') as wf:\n",
    "    #     pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # print(\"{}: finished\".format(f))\n",
    "            \n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != 'E3SM-2-0':\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    all_files = os.listdir(new_path)\n",
    "    s_files = [file for file in all_files if file.startswith(\"so_Omon\")]\n",
    "    t_files = [file for file in all_files if file.startswith(\"thetao_Omon\")]\n",
    "\n",
    "    s_files.sort()\n",
    "    t_files.sort()\n",
    "    \n",
    "    # Iterate through the pairs of files\n",
    "    fn = 0\n",
    "    for s_file, t_file in zip(s_files, t_files):\n",
    "        s_file_path = os.path.join(new_path, s_file)\n",
    "        t_file_path = os.path.join(new_path, t_file)\n",
    "        \n",
    "        s_longname = s_file.split('_')[1:]\n",
    "        t_longname = t_file.split('_')[1:]\n",
    "        \n",
    "        if s_longname != t_longname:\n",
    "            print('time span does not match ')\n",
    "            break\n",
    "        else:\n",
    "            chunksize = {\"time\":12}\n",
    "\n",
    "            dss = xr.open_dataset(s_file_path, chunks=chunksize)\n",
    "            dst = xr.open_dataset(t_file_path, chunks=chunksize)\n",
    "\n",
    "            levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "\n",
    "            nfns = np.linspace(0, len(dss.so.time), 2) #int(len(dss.so.time)/12+1))\n",
    "\n",
    "            for nfn in range(0, len(nfns)-1):\n",
    "                sfn = fn + nfn\n",
    "                savepath = 'data_mld0/' + f + '_' + str(sfn) + '.pickle'\n",
    "                # if os.path.exists(savepath):\n",
    "                #     continue\n",
    "                print('file {}, number {}, time {} - {}'.format(s_file, sfn, nfns[nfn], nfns[nfn+1]))\n",
    "\n",
    "                das = dss.so.isel(time = slice(int(nfns[nfn]), int(nfns[nfn+1])))\n",
    "                dat = dst.thetao.isel(time = slice(int(nfns[nfn]), int(nfns[nfn+1])))\n",
    "\n",
    "                da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "                \n",
    "                if 'units' in das[levname].attrs:\n",
    "                    if das[levname].units == 'centimeters':\n",
    "                        da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "                        \n",
    "                \n",
    "                if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "                    if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "                        print(\"{} doesn't have regular grid.\".format(f))\n",
    "                        continue\n",
    "                    else:\n",
    "                        da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "                        # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "                        pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "                        plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "                        pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "                else:\n",
    "                    da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "                    pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "                    plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "                \n",
    "                if len(np.shape(pltx)) > 2:\n",
    "                    pltx = pltx.isel(time = 0)\n",
    "                    plty = plty.isel(time = 0)\n",
    "                    \n",
    "                da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "                da_mld = cal_mld(da_sep, levname)\n",
    "                da_mld_new = da_mld.load()\n",
    "                break\n",
    "\n",
    "                # with open(savepath, 'wb') as wf:\n",
    "                #     pickle.dump(da_mld_new, wf, pickle.HIGHEST_PROTOCOL)\n",
    "            fn = sfn + 1\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mld_new.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sigma0.isel(time=0, lev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = 'E3SM-2-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newfn in range(0, sfn+1):\n",
    "    picklefilename = 'data_mld0/' + model1 + '_' + str(newfn) + '.pickle'\n",
    "    with open(picklefilename, 'rb') as pf:\n",
    "        mld_data0 = pickle.load(pf)\n",
    "    if newfn == 0:\n",
    "        da_mld_all = mld_data0\n",
    "    else:\n",
    "        da_mld_all = xr.concat([da_mld_all, mld_data0], dim=\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldmax = da_mld_all.max(\"time\")\n",
    "mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "\n",
    "newd = {\"mldmax\":mldmax, \n",
    "        \"mld2kfq\":mld2000_frq,\n",
    "        \"pltx\":pltx, \n",
    "        \"plty\":plty}\n",
    "\n",
    "\n",
    "datasavepath = 'data_mld/' + model1 + '.pickle'\n",
    "with open(datasavepath, 'wb') as wf:\n",
    "        pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = 'CanESM5-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != model1:\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    all_files = os.listdir(new_path)\n",
    "    s_files = [file for file in all_files if file.startswith(\"so_Omon\")]\n",
    "    t_files = [file for file in all_files if file.startswith(\"thetao_Omon\")]\n",
    "\n",
    "    s_files.sort()\n",
    "    t_files.sort()\n",
    "    \n",
    "    # Iterate through the pairs of files\n",
    "    fn = 0\n",
    "    for s_file, t_file in zip(s_files, t_files):\n",
    "        savepath = 'data_mld0/' + f + '_' + str(fn) + '.pickle'\n",
    "        if os.path.exists(savepath):\n",
    "            fn += 1\n",
    "            continue\n",
    "\n",
    "        s_file_path = os.path.join(new_path, s_file)\n",
    "        t_file_path = os.path.join(new_path, t_file)\n",
    "        \n",
    "        s_longname = s_file.split('_')[1:]\n",
    "        t_longname = t_file.split('_')[1:]\n",
    "        \n",
    "        if s_longname == t_longname:\n",
    "\n",
    "            chunksize = {\"time\":12}\n",
    "\n",
    "            dss = xr.open_dataset(s_file_path, chunks=chunksize)\n",
    "            dst = xr.open_dataset(t_file_path, chunks=chunksize)\n",
    "\n",
    "            das = dss.so\n",
    "            dat = dst.thetao\n",
    "\n",
    "            if 'type' in das.coords:\n",
    "                das = das.reset_coords('type', drop = True)\n",
    "                dat = dat.reset_coords('type', drop = True)\n",
    "\n",
    "            \n",
    "            da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "\n",
    "            levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "            if 'units' in das[levname].attrs:\n",
    "                if das[levname].units == 'centimeters':\n",
    "                    da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "\n",
    "            if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "                if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "                    print(\"{} doesn't have regular grid.\".format(f))\n",
    "                    continue\n",
    "                else:\n",
    "                    da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "                    # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "                    pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "                    plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "                    pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "            else:\n",
    "                da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "                pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "                plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "                \n",
    "\n",
    "            if len(np.shape(pltx)) > 2:\n",
    "                pltx = pltx.isel(time = 0)\n",
    "                plty = plty.isel(time = 0)\n",
    "\n",
    "            da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "            da_mld = cal_mld(da_sep, levname)\n",
    "            da_mld_new = da_mld.load()\n",
    "\n",
    "            with open(savepath, 'wb') as wf:\n",
    "                pickle.dump(da_mld_new, wf, pickle.HIGHEST_PROTOCOL)\n",
    "            fn += 1\n",
    "        else:\n",
    "            print(s_longname)\n",
    "            print(t_longname)\n",
    "            break\n",
    "            # if fn == 0:\n",
    "            #     da_mld_all = da_mld_new.load()\n",
    "            # else:\n",
    "            #     da_mld_all = xr.concat([da_mld_all, da_mld_new], dim=\"time\")\n",
    "            # if fn > 2:\n",
    "            #     break\n",
    "            # fn = fn + 1\n",
    "            \n",
    "        \n",
    "\n",
    "    # mldmax = da_mld.max(\"time\")\n",
    "    # mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "    # mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "    \n",
    "    # newd = {\"mldmax\":mldmax.load(), \n",
    "    #         \"mld2kfq\":mld2000_frq.load(), \n",
    "    #         \"pltx\":pltx, \n",
    "    #         \"plty\":plty}\n",
    "    \n",
    "    # with open(fpath, 'wb') as wf:\n",
    "    #     pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # print(\"{}: finished\".format(f))\n",
    "            \n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newfn in range(0, fn):\n",
    "    picklefilename = 'data_mld0/' + model1 + '_' + str(newfn) + '.pickle'\n",
    "    with open(picklefilename, 'rb') as pf:\n",
    "        mld_data0 = pickle.load(pf)\n",
    "    if newfn == 0:\n",
    "        da_mld_all = mld_data0\n",
    "    else:\n",
    "        da_mld_all = xr.concat([da_mld_all, mld_data0], dim=\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldmax = da_mld_all.max(\"time\")\n",
    "mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "\n",
    "newd = {\"mldmax\":mldmax, \n",
    "        \"mld2kfq\":mld2000_frq,\n",
    "        \"pltx\":pltx, \n",
    "        \"plty\":plty}\n",
    "\n",
    "\n",
    "datasavepath = 'data_mld/' + model1 + '.pickle'\n",
    "with open(datasavepath, 'wb') as wf:\n",
    "        pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(path_data):\n",
    "    if f != 'GISS-E2-2-H':\n",
    "        continue\n",
    "    new_path = path_data + '/' + f\n",
    "    fpath = 'data_mld/' + f + '.pickle'\n",
    "    # print(new_path)\n",
    "\n",
    "    sf = new_path + \"/so*.nc\"\n",
    "    tf = new_path + \"/thetao*.nc\"\n",
    "    # print(new_sof)\n",
    "    dss = xr.open_mfdataset(sf)\n",
    "    dst = xr.open_mfdataset(tf)\n",
    "\n",
    "    das = dss.so\n",
    "    dat = dst.thetao\n",
    "    \n",
    "    if 'type' in das.coords:\n",
    "        das = das.reset_coords('type', drop = True)\n",
    "        dat = dat.reset_coords('type', drop = True)\n",
    "    \n",
    "    da_sigma0 = gsw.sigma0(das.where(das > 0), dat.where(dat != 0))\n",
    "\n",
    "    levname = datapd.loc[datapd['source_id'] == f]['zname'].values[0]\n",
    "    if 'units' in das[levname].attrs:\n",
    "        if das[levname].units == 'centimeters':\n",
    "            da_sigma0[levname] = da_sigma0[levname]/100 # unit: cm --> m\n",
    "\n",
    "    if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['latname']):\n",
    "        if pd.isna(datapd.loc[datapd['source_id'] == f].iloc[0]['xname']):\n",
    "            print(\"{} doesn't have regular grid.\".format(f))\n",
    "            continue\n",
    "        else:\n",
    "            da_south = da_sigma0.sel({datapd.loc[datapd['source_id'] == f].iloc[0]['yname']: slice(-90, -50)})\n",
    "            # da_south = da_sigma0.where(da_sigma0[datapd.at[i, 'xname']]< -50, drop=True)\n",
    "            pltx0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['xname']]\n",
    "            plty0 = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['yname']]\n",
    "            pltx, plty = np.meshgrid(pltx0, plty0)\n",
    "    else:\n",
    "        da_south = da_sigma0.where(da_sigma0[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']] < -50, drop=True)\n",
    "        pltx = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['lonname']].load()\n",
    "        plty = da_south[datapd.loc[datapd['source_id'] == f].iloc[0]['latname']].load()\n",
    "    \n",
    "    if len(np.shape(pltx)) > 2:\n",
    "        pltx = pltx.isel(time = 0)\n",
    "        plty = plty.isel(time = 0)\n",
    "\n",
    "    da_sep = list(da_south.groupby('time.month'))[8][-1]\n",
    "    da_mld = cal_mld(da_sep, levname)\n",
    "    \n",
    "    mldmax = da_mld.max(\"time\")\n",
    "    mld2000 = da_mld.where(da_mld >= 2000).count('time')\n",
    "    mld2000_frq = mld2000.where(mld2000>0)/len(da_mld.time)\n",
    "    \n",
    "    newd = {\"mldmax\":mldmax.load(), \n",
    "            \"mld2kfq\":mld2000_frq.load(), \n",
    "            \"pltx\":pltx, \n",
    "            \"plty\":plty}\n",
    "    \n",
    "    with open(fpath, 'wb') as wf:\n",
    "        pickle.dump(newd, wf, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(\"{}: finished\".format(f))\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd.loc[datapd['source_id'] == f].iloc[0]['xname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intake import open_esm_datastore\n",
    "col = open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(variable_id = ['thetao', 'so'], experiment_id = 'piControl')\n",
    "nlist = cat.df.source_id.unique()\n",
    "collist = list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df.to_csv(\"dataset_st_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(variable_id = ['thetao', 'so'], experiment_id = 'piControl', source_id = 'EC-Earth3-LR')\n",
    "cat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df.zstore[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_zarr(gcs.get_mapper(\"gs://cmip6/CMIP6/CMIP/NUIST/NESM3/piControl/r1i1p1f1/SImon/siconc/gn/v20190704/\"), consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_zarr(gcs.get_mapper(cat.df.zstore[1]), consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "col = intake.open_esm_datastore(\n",
    "    \"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\"\n",
    ")\n",
    "cat = col.search(variable_id = ['thetao', 'so'], experiment_id = 'piControl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\n",
    "    \"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat['CMIP.EC-Earth-Consortium.EC-Earth3-LR.piControl.Omon.gn'].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, int(len(datapd)/2)):\n",
    "    if datapd.at[2*i, 'source_id'] == datapd.at[2*i+1, 'source_id']:\n",
    "        if datapd.at[2*i, 'member_id'] == datapd.at[2*i+1, 'member_id']:\n",
    "            if datapd.at[2*i, 'grid_label'] == datapd.at[2*i+1, 'grid_label']:\n",
    "                ds1 = xr.open_zarr(gcs.get_mapper(datapd.at[2*i,'zstore']), consolidated=True)\n",
    "                # ds2 = xr.open_zarr(gcs.get_mapper(datapd.at[2*i+1,'zstore']), consolidated=True)\n",
    "                print(datapd.at[2*i, 'source_id'], ' ', datapd.at[2*i, 'grid_label'])\n",
    "                print(ds1[datapd.at[2*i, 'variable_id']], '\\n')\n",
    "                \n",
    "            else:\n",
    "                print(i)\n",
    "        else:\n",
    "            print(i)\n",
    "    else:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
